
[{"content":" A classification technique which computes the probability or likelihood that the data belongs to a class of interest. $$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n$$\nGoal : Maximize likelihood\nOptimization Method : Gradient Descent\nGenerally fits an ‚ÄòS‚Äô curve (‚ÄòS‚Äô shaped logistic function) for the data to belong to separate classes.\nBinary Classification # Sigmoid transforms the linear combination of inputs into probability. $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n\\(z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\\) The curve which gives the maximum likelihood is selected as the end result.\nMultinomial Logistic Regression # One-vs-Rest (OvR) Classification :\nFor \\(K\\) classes, train \\(K\\) separate binary classifiers (one class vs the rest). For a new observation, output \\(K\\) probabilities and select the class with the highest probability. Different binary classifiers may produce overlapping decision regions which may lead to ambiguous classification, especially when classes are imbalanced. Softmax Logistic Function :\nSoftmax ensures the predicted probabilities of all classes sum to 1. $$P(y=k|x)=\\frac{e^{x^T\\beta_{k}}}{\\sum_{j=1}^Ke^{x^T\\beta_{j}}}$$\n\\(x\\) = feature vector \\(K\\) = number of classes \\(\\beta_{k}\\) = parameter vector for class \\(k\\) Provides a coherent probability model. Model coefficients can be interpreted as the change in log-odds of being in a given class. Maximum Likelihood Estimation # Binary Classification : $$L(\\beta) = \\prod_{i=1}^{N} p_i^{y_i} (1-p_i)^{1-y_i}$$\n\\(p_i\\) = predicted probability for observation (\\phi) \\(y_i\\) = actual label (0 or 1) Log Transformation (Log-Likelihood) :\nThis results in underflow for large datasets since all values are between 0 and 1. We apply log transformation to change it into a summation. $$\\ell(\\beta)=\\sum_{i=1}^{N}(y_i \\log(p_i) + (1-y_i) \\log(1-p_i))$$ Multi-Classification using Softmax : $$\\ell({\\beta_{k}}) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log\\left( \\frac{e^{x_i^\\top \\beta_{k}}}{\\sum_{j=1}^{K} e^{x_i^\\top \\beta_j}} \\right)$$\nAssumptions # An S shaped curve is assumed to fit the data. The independent variable should have strong correlation with the likelihood of the class. Linearity in the Logit : The log-odds (logits) of the outcome are assumed to be a linear combination of the independent variables. $$\\log\\left( \\frac{p}{1-p} \\right)=\\beta_{0}+\\beta_{1}x_{1}+\\dots+\\beta_{n}x_{n}$$ The coefficients are in the log-odds scale. All Linear Regression tests can be done on this line. Independence : Observations are assumed to be independent of each other. No or Little Multicollinearity : The independent variables should not be highly correlated with each other. A sufficiently large sample is preferred to obtain reliable estimates. Limitations # Linear Decision Boundary in Logit Space : Assuming a linear relationship between the predictors and the log-odds of the outcome, might not capture more complex, non-linear relationships. Sensitivity to Outliers : Extreme values can disproportionately affect the model. Multicollinearity : High correlation among independent variables can destabilize coefficient estimates. Feature transformations : For data that is not linearly separable (in the logit domain), performance may suffer unless features are transformed or interaction terms are added. Diagnostic Decision Table # Observation Implication Action Poor Overall Model Fit : High deviance, low pseudo R-squared, or significant lack-of-fit (e.g., failing the Hosmer-Lemeshow test). The model may be mis-specified, missing important predictors, or not capturing the true relationship between variables. Reassess model specification: consider adding relevant predictors, interaction terms, or non-linear transformations. Non-significant Coefficients : Predictors with high p-values or wide confidence intervals. These predictors may not contribute meaningfully to predicting the outcome, possibly diluting the model‚Äôs effectiveness. Remove or re-specify insignificant variables, and ensure that only meaningful predictors remain in the model. Low Predictive Performance : Low classification accuracy, poor ROC-AUC, or imbalanced confusion matrix (e.g., high false positives/negatives). The model might be underfitting, failing to capture important data patterns, or struggling with class imbalance. Perform feature engineering, consider re-balancing the dataset, explore alternative model specifications, or try regularization techniques. High Multicollinearity : Indicators such as high Variance Inflation Factor (VIF) values or inflated standard errors. Predictor variables are highly correlated, leading to unstable coefficient estimates and difficulty in interpreting their individual effects. Remove, combine, or transform correlated predictors, or apply regularization (L1/L2) to mitigate multicollinearity issues. Influential Observations : A few data points exhibit high Cook‚Äôs distance or leverage values. These observations may disproportionately affect model estimates, potentially skewing results and interpretations. Investigate and validate these outliers. Consider robust regression techniques or, if justified, remove problematic observations after careful analysis. Patterned Residuals : Residual plots show systematic patterns or non-random distribution. This suggests that the model is missing key relationships (e.g., interactions or non-linear effects), indicating mis-specification. Consider adding interaction terms, polynomial terms, or applying variable transformations to better capture the underlying data structure. Poor Calibration in Multi-Class Models (OvR approach) : Inconsistent or poorly calibrated predicted probabilities across classes. The individual binary classifiers may not provide directly comparable probabilities, potentially leading to ambiguous class assignments. For multi-class problems, consider switching to multinomial (softmax) logistic regression, applying probability calibration techniques, or using methods to better balance class predictions. Steps # Goal : Maximize likelihood\nModel : \\(\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\\)\nCompute the likelihood for each observation. Apply the log transformation to obtain the log-likelihood. Optimize the parameters using Gradient Descent. Calculate evaluation metrics and diagnose. Use Wald‚Äôs Test to determine whether a feature is useful in computing the prediction. Validate model assumptions such as linearity and independence. ","date":"February 14 2025","externalUrl":null,"permalink":"/posts/logistic-regression/","section":"Posts","summary":"","title":"Logistic Regression","type":"posts"},{"content":" Definition # A model that fits a linear equation to the data. $$ \\hat{y}=w^Tx+\\phi $$ where \\( w \\) = weight vector, \\( x \\) = feature vector, \\( \\phi \\) = bias\nResidual : \\(r_i=y_i-\\hat{y_{i}}=y_{i}-(w^Tx_{i}+\\phi)\\)\nLoss Function : Mean Squared Error\nOptimization Method : Gradient Descent\nAssumptions # Linearity: Dependent and independent variables have linear relationship. Check for scatterplot trends. Curved/parabolic lines indicate non-linear relationships. Independence: Observations are independent of each other. Homoscedasticity: Constant variance of residuals. Normality: Residuals are normally distributed. No Multicollinearity: Independent variables are not highly correlated. Limitations # Non-Linear Relationships : Cannot model non-linear relationships. Use polynomial regression or apply transformations. Outliers : Sensitive to outliers. Detect and remove outliers using IQR or Z-score analysis. Categorical Features : Does not handle categorical features directly. Use One-Hot Encoding for nominal features or Ordinal Encoding for ordered features. High Multicollinearity - Inflates variance and makes coefficients unreliable. Check VIF or remove redundant features using PCA or reduce overfitting using Ridge or Lasso Regression. Heteroscedasticity - Unequal variance will lead to uncertain predictions and biased hypothesis tests. Apply log transformations or WLS regression. Non Normal Residuals - Leads to unreliable p-values and confidence intervals. Apply Box-Cox transformation or use Quantile Regression High Dimensionality : Apply PCA or feature selection. Features with weak correlation may be removed. Diagnostic Decision Table # Observation Implication Action High feature-feature correlation Multicollinearity risk Calculate VIF, drop/reduce features or use Ridge Regression Strong feature-target correlation Good predictor candidate Prioritize in model. Check for data leakage Non-linear patterns Violates linearity assumption Add polynomial terms or transformations Bimodal distributions Potential subgroups in data Investigate data collection. Try separate models for each subgroup or use interactions Outliers in pairplot Potential data errors/edge cases Apply robust scaling or winsorization Heteroscedasticity Violates constant variance assumption Use log transformation or weighted regression Non-normal residuals Violates normality assumption Apply Box-Cox transformation or increase data Autocorrelation in residuals Violates independence assumption Use Durbin-Watson test; consider lag variables High leverage points Can disproportionately affect model Detect using Cook‚Äôs Distance and consider removal Omitted variable bias Model missing important predictors Include relevant features or domain knowledge Irrelevant features Increases model complexity, reduces performance Perform feature selection (LASSO, backward elimination) Steps # Goal: Minimize MSE\nCheck all assumptions. Split data into training and testing sets. Initialize random weights for each feature. Calculate predictions. Calculate MSE. Calculate steps using Gradient Descent Evaluate using \\(R^2\\): If \\(R^2\\) is low, check for missing predictor or non-linearity. If \\(R^2\\) is too high, possible overfitting (Adjusted \\(R^2\\)) Calculate p-value for \\(R^2\\) using F-Test. Plot feature coefficients, residual distribution and actual vs predicted values scatter plot. Code # import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from statsmodels.stats.outliers_influence import variance_inflation_factor from scipy.stats import shapiro, f_oneway import statsmodels.api as sm # Load dataset df = pd.read_csv(\u0026#39;winequality-red.csv\u0026#39;, sep=\u0026#39;;\u0026#39;) # Define target column target_column = \u0026#39;quality\u0026#39; # Step 1: Validate Assumptions Before Applying Linear Regression # 1.1 Check for missing values if df.isnull().sum().sum() \u0026gt; 0: print(\u0026#34;‚ö†Ô∏è Dataset contains missing values. Handle them before proceeding.\u0026#34;) # 1.2 Check multicollinearity using Variance Inflation Factor (VIF) X = df.drop(columns=[target_column]) y = df[target_column] vif_data = pd.DataFrame() vif_data[\u0026#34;Feature\u0026#34;] = X.columns vif_data[\u0026#34;VIF\u0026#34;] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] if vif_data[\u0026#34;VIF\u0026#34;].max() \u0026gt; 10: print(\u0026#34;‚ö†Ô∏è High multicollinearity detected. Consider removing/reducing features using PCA, Ridge Regression, or feature selection.\u0026#34;) # 1.3 Check linearity assumption using a correlation heatmap plt.figure(figsize=(10, 8)) sns.heatmap(df.corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Feature Correlation Heatmap\u0026#34;) plt.show() # 1.4 Check residual normality using the Shapiro-Wilk test (valid for small samples) _, p_value = shapiro(df[target_column]) if p_value \u0026lt; 0.01: print(\u0026#34;‚ö†Ô∏è Residuals are not normally distributed. Consider log transformation or robust regression.\u0026#34;) # 1.5 Check homoscedasticity using ANOVA _, p_homo = f_oneway(*[df[target_column][df[col] \u0026gt; df[col].median()] for col in df.drop(columns=[target_column]).columns]) if p_homo \u0026lt; 0.05: print(\u0026#34;‚ö†Ô∏è Heteroscedasticity detected. Consider applying log transformation or using Weighted Least Squares (WLS).\u0026#34;) print(\u0026#34;Proceeding with Linear Regression...\u0026#34;) # Step 2: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Step 3: Train Linear Regression Model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Predictions y_pred = model.predict(X_test) # Step 5: Evaluate Model rmse = np.sqrt(mean_squared_error(y_test, y_pred)) r2 = r2_score(y_test, y_pred) print(f\u0026#34;\\nüìä Model Performance:\\nRMSE: {rmse:.3f}\\nR-squared (R¬≤): {r2:.3f}\u0026#34;) # Step 6: Feature Importance feature_importance = pd.DataFrame({\u0026#39;Feature\u0026#39;: X_train.columns, \u0026#39;Coefficient\u0026#39;: model.coef_}) feature_importance = feature_importance.sort_values(by=\u0026#39;Coefficient\u0026#39;, ascending=False) plt.figure(figsize=(10, 5)) sns.barplot(x=\u0026#39;Coefficient\u0026#39;, y=\u0026#39;Feature\u0026#39;, data=feature_importance, palette=\u0026#39;coolwarm\u0026#39;) plt.title(\u0026#34;Feature Importance in Linear Regression\u0026#34;) plt.show() # Step 7: Residual Analysis residuals = y_test - y_pred plt.figure(figsize=(8, 5)) sns.histplot(residuals, kde=True, bins=30) plt.title(\u0026#34;Residuals Distribution\u0026#34;) plt.xlabel(\u0026#34;Residuals\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_pred, y=residuals, alpha=0.5) plt.axhline(y=0, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#34;Predicted Quality\u0026#34;) plt.ylabel(\u0026#34;Residuals\u0026#34;) plt.title(\u0026#34;Residuals vs Predicted Values\u0026#34;) plt.show() # Step 8: Actual vs Predicted Visualization plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_test, y=y_pred, alpha=0.6) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Perfect fit line plt.xlabel(\u0026#34;Actual Quality\u0026#34;) plt.ylabel(\u0026#34;Predicted Quality\u0026#34;) plt.title(\u0026#34;Actual vs. Predicted Wine Quality\u0026#34;) plt.show() # Step 9: Model Statistics (p-value for R¬≤) X_train_sm = sm.add_constant(X_train) # Add constant for intercept ols_model = sm.OLS(y_train, X_train_sm).fit() print(\u0026#34;\\nüìä Model Summary:\u0026#34;) print(ols_model.summary()) # If R¬≤ is too high, check for overfitting adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1) if adjusted_r2 \u0026lt; r2 - 0.05: print(\u0026#34;‚ö†Ô∏è Possible overfitting detected. Consider cross-validation or feature selection.\u0026#34;) ","date":"January 30 2025","externalUrl":null,"permalink":"/posts/linear-regression/","section":"Posts","summary":"","title":"Linear Regression","type":"posts"},{"content":" ResNet Block # 3x3 Convolution (stride=2) - Downsamples the image into half the height and width. Also, doubles the number of channels (number of filters are doubled). Sliding a 3x3 kernel/matrix onto the image and dot product is calculated with overlapped images. 3x3 is small enough to capture fine details and more efficient. Can be stacked to capture complex patterns. Batch Norm ReLU Thought Process # Our goal is to increase the resolution of the image.\nIf the model is trained on a vanilla deep net with both low and high resolution input images, it will increase our training loss since the input signal is being lost when its passed through a lot of layers.\nOur goal is to now learn the residual to be added to the image to increase its resolution.\nThe idea is to add the input image to our residual at the end of our block. Two problems arise:\nDimension mismatch (When the input and residual have different dimensions, we cannot add) A steady stream of input data across the network So we add residual connections to handle these. And we treat the network as a series of residual blocks instead of layers.\nEach block does not take penalty from the loss function since it can output the same identity function.\nThis enables very deep networks.\nCode # def forward(self, x: Tensor) -\u0026gt; Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out Resnet18, 34, 50, 101, 152. These are pretrained models and the number indicates the number of layers in the architecture.\nArchitecture # ResNet models introduce the concept of residual learning, where the network learns residual functions with reference to the layer inputs, rather than learning unreferenced functions. This allows the model to train very deep networks effectively.\nUse Cases # ResNet models are commonly used in image classification tasks and are known for their performance on large-scale datasets such as ImageNet.\nStrengths and Weaknesses # Strengths: Effective in training very deep networks. Reduces the problem of vanishing gradients. Weaknesses: Computationally expensive for very deep versions like ResNet-152. May not be as efficient for smaller datasets or less complex tasks. Papers # He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778*. ","date":"January 9 2025","externalUrl":null,"permalink":"/posts/resnet/","section":"Posts","summary":"","title":"What is ResNet?","type":"posts"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/linear-models/","section":"Tags","summary":"","title":"Linear Models","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/","section":"Siddanth Emani Blog","summary":"","title":"Siddanth Emani Blog","type":"page"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"Supervised Learning","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/regression/","section":"Tags","summary":"","title":"Regression","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/resnet/","section":"Tags","summary":"","title":"ResNet","type":"tags"},{"content":"","externalUrl":null,"permalink":"/about/","section":"Siddanth Emani Blog","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]