[{"content":" ResNet Block # 3x3 Convolution (stride=2) - Downsamples the image into half the height and width. Also, doubles the number of channels (number of filters are doubled). Sliding a 3x3 kernel/matrix onto the image and dot product is calculated with overlapped images. 3x3 is small enough to capture fine details and more efficient. Can be stacked to capture complex patterns. Batch Norm ReLU Thought Process # Our goal is to increase the resolution of the image.\nIf the model is trained on a vanilla deep net with both low and high resolution input images, it will increase our training loss since the input signal is being lost when its passed through a lot of layers.\nOur goal is to now learn the residual to be added to the image to increase its resolution.\nThe idea is to add the input image to our residual at the end of our block. Two problems arise:\nDimension mismatch (When the input and residual have different dimensions, we cannot add) A steady stream of input data across the network So we add residual connections to handle these. And we treat the network as a series of residual blocks instead of layers.\nEach block does not take penalty from the loss function since it can output the same identity function.\nThis enables very deep networks.\nCode # def forward(self, x: Tensor) -\u0026gt; Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out Resnet18, 34, 50, 101, 152. These are pretrained models and the number indicates the number of layers in the architecture.\nArchitecture # ResNet models introduce the concept of residual learning, where the network learns residual functions with reference to the layer inputs, rather than learning unreferenced functions. This allows the model to train very deep networks effectively.\nUse Cases # ResNet models are commonly used in image classification tasks and are known for their performance on large-scale datasets such as ImageNet.\nStrengths and Weaknesses # Strengths: Effective in training very deep networks. Reduces the problem of vanishing gradients. Weaknesses: Computationally expensive for very deep versions like ResNet-152. May not be as efficient for smaller datasets or less complex tasks. Papers # He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778*. ","date":"January 9 2025","externalUrl":null,"permalink":"/posts/resnet/","section":"Posts","summary":"","title":"What is ResNet?","type":"posts"},{"content":" Sigmoid # $$\\sigma(x) = \\frac{1} {1 + e^{-x}}$$ $$\\sigma\u0026rsquo;(x)=\\sigma(x)(1-\\sigma(x))$$ Always outputs the number between 0 and 1.\nUse Cases : Binary Classification (Logistic Regression), Feed Forward Neural Networks\nclass CustomSigmoid(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -\u0026gt; torch.Tensor: output = 1 / (1 + torch.exp(-input)) ctx.save_for_backward(output) return output @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: output, = ctx.saved_tensors grad_input = grad_output * output * (1 - output) return grad_input def sigmoid(x: torch.Tensor) -\u0026gt; torch.Tensor: return CustomSigmoid.apply(x) Tanh # $$\\tanh=\\frac{sinh}{\\cosh}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ $$\\tanh\u0026rsquo;(x)=1-\\tanh^2(x)$$ Always outputs the number between -1 and 1\nUse Cases : Used in hidden layers since it is zero-centered and the gradients are stronger.\nclass CustomTanh(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -\u0026gt; torch.Tensor: exp_x = torch.exp(input) exp_neg_x = torch.exp(-input) tanh = (exp_x - exp_neg_x) / (exp_x + exp_neg_x) ctx.save_for_backward(tanh) return tanh @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: tanh, = ctx.saved_tensors grad_input = grad_output * (1 - tanh ** 2) return grad_input def tanh(x: torch.Tensor) -\u0026gt; torch.Tensor: return CustomTanh.apply(x) ReLU # $$Relu(x)=max(0,x)$$ $$\\text{relu}\u0026rsquo;(x) = \\begin{cases} 1 \u0026amp; \\text{if } x \u0026gt; 0, \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nReplaces every negative number to zero.\nUse Cases: Deep Learning models, especially in hidden layers.\nTraining with saturating non-linearities like Sigmoid and Tanh is slower than using non-saturating non-linearities like ReLU.\nThere is a non-differentiable kink at x=0.\nclass CustomReLU(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -\u0026gt; torch.Tensor: ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input \u0026lt; 0] = 0 return grad_input def relu(x: torch.Tensor) -\u0026gt; torch.Tensor: return CustomReLU.apply(x) Softplus # $$softplus(x)=\\log(1+e^x)$$ A smooth approximation of ReLU. Larger negative values are close to zero. Positive values behave almost linearly.\nKey benefit is it is continuously differentiable everywhere and is equal to Sigmoid $$\\frac{d}{dx}softplus(x)=\\frac{1}{1+e^{-x}}=\\sigma(x)$$ This is better for optimization during training since it avoids the non-differentiable kink at \\(x=0\\) present in ReLU.\nclass CustomSoftplus(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -\u0026gt; torch.Tensor: softplus = torch.log(1 + torch.exp(input)) ctx.save_for_backward(input) return softplus @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: input, = ctx.saved_tensors grad_input = grad_output * (1 / (1 + torch.exp(-input))) return grad_input def softplus(x: torch.Tensor) -\u0026gt; torch.Tensor: return CustomSoftplus.apply(x) Argmax # $$\\text{argmax}_xf(x)=x^* \\ \\ where \\ \\ f(x^*)\\geq f(x)\\ \\ \\ \\forall \\ x$$\nReturns the input value at which a function reaches the maximum.\nSince it‚Äôs a discrete selection operation, it is not differentiable in the conventional sense.\nUse Cases: Used in the output layer of a neural network for multi-class classification tasks.\nclass CustomArgmax(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor, dim: int) -\u0026gt; torch.Tensor: # Find the maximum value\u0026#39;s index along the specified dimension max_indices = input.max(dim=dim)[1] ctx.save_for_backward(input, max_indices, dim) return max_indices @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: # Non-differentiable, so return zero gradient input, _, _ = ctx.saved_tensors return torch.zeros_like(input), None # None for dim def argmax(x: torch.Tensor, dim: int) -\u0026gt; torch.Tensor: return CustomArgmax.apply(x, dim) Softmax # $$\\sigma(z_{i})=\\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}}\\ \\ \\ for\\ i=1,2,\\ldots K$$ Converts a vector of K real numbers into a probability distribution [0 to 1] of K possible outcomes.\nIt preserves the order or ranking of the original input values.\nDerivative (Jacobian Matrix): \\(\\frac{d\\sigma(z_{i})}{dz_{j}}=\\sigma(z_{i})(\\delta_{ij}-\\sigma(z_{j}))\\) where \\(\\delta_{ij}\\) is the Kronecker delta (1 if \\(i=j\\) and 0 otherwise.)\nUse cases : Multi-class single label classification\nclass CustomSoftmax(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor, dim: int) -\u0026gt; torch.Tensor: exp_x = torch.exp(input) sum_exp_x = exp_x.sum(dim=dim, keepdim=True) softmax = exp_x / sum_exp_x ctx.save_for_backward(softmax, dim) return softmax @staticmethod def backward(ctx, grad_output: torch.Tensor) -\u0026gt; torch.Tensor: softmax, dim = ctx.saved_tensors grad_input = softmax * (grad_output - (softmax * grad_output).sum(dim=dim, keepdim=True)) return grad_input, None # None for dim def softmax(x: torch.Tensor, dim: int) -\u0026gt; torch.Tensor: return CustomSoftmax.apply(x, dim) Hierarchical Softmax # Hierarchical Softmax is an efficient alternative to standard softmax for handling large output spaces. It reduces the computational cost from O(N) to O(log N) by organizing the classes into a binary tree. At each internal node, a sigmoid function is applied to determine the traversal path.\nProbability is the product of sigmoid outputs along the path to a class.\nUse Cases: Large output spaces like language modeling.\nimport torch import torch.nn as nn import torch.nn.functional as F class HierarchicalSoftmax(nn.Module): def __init__(self, input_dim: int, num_classes: int): \u0026#34;\u0026#34;\u0026#34; Initialize the Hierarchical Softmax layer. Args: input_dim (int): Dimension of the input features. num_classes (int): Number of output classes. \u0026#34;\u0026#34;\u0026#34; super(HierarchicalSoftmax, self).__init__() self.num_classes = num_classes self.input_dim = input_dim # Number of internal nodes in a balanced binary tree self.num_internal = num_classes - 1 # Learnable parameters for internal nodes self.weights = nn.Parameter(torch.randn(self.num_internal, input_dim)) self.biases = nn.Parameter(torch.zeros(self.num_internal)) # Precompute tree structure and paths self.tree, self.paths = self._build_tree_and_paths(num_classes) def _build_tree_and_paths(self, num_classes: int): \u0026#34;\u0026#34;\u0026#34; Build a balanced binary tree and precompute paths for each class. Args: num_classes (int): Number of leaf nodes (classes). Returns: tuple: (tree structure, paths dictionary) \u0026#34;\u0026#34;\u0026#34; tree = {} paths = {} # Construct the tree (0-based indexing for internal nodes) for i in range(self.num_internal): left = 2 * i + 1 right = 2 * i + 2 tree[i] = [left, right] if right \u0026lt; num_classes else [left] # Precompute paths for each class for class_idx in range(num_classes): path = [] node = class_idx + self.num_internal # Leaf node index while node \u0026gt; 0: parent = (node - 1) // 2 if tree[parent][0] == node: path.append((parent, 0)) # Left child else: path.append((parent, 1)) # Right child node = parent paths[class_idx] = list(reversed(path)) # Root-to-leaf path return tree, paths def forward(self, x: torch.Tensor, targets: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Compute log probabilities for a batch of inputs and targets. Args: x (torch.Tensor): Input tensor of shape (batch_size, input_dim). targets (torch.Tensor): Target classes of shape (batch_size,). Returns: torch.Tensor: Log probabilities of shape (batch_size,). \u0026#34;\u0026#34;\u0026#34; batch_size = x.size(0) log_probs = torch.zeros(batch_size, device=x.device) for i in range(batch_size): # Get precomputed path for the target class path = self.paths[targets[i].item()] prob = 1.0 # Compute probability along the path for node, direction in path: logit = torch.matmul(x[i], self.weights[node]) + self.biases[node] sigmoid = torch.sigmoid(logit) prob *= sigmoid if direction == 0 else (1 - sigmoid) # Compute log probability with numerical stability log_probs[i] = torch.log(prob + 1e-10) return log_probs def predict(self, x: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Predict class indices for a batch of inputs. Args: x (torch.Tensor): Input tensor of shape (batch_size, input_dim). Returns: torch.Tensor: Predicted class indices of shape (batch_size,). \u0026#34;\u0026#34;\u0026#34; batch_size = x.size(0) predictions = [] for i in range(batch_size): node = 0 # Start at root while node \u0026lt; self.num_internal: logit = torch.matmul(x[i], self.weights[node]) + self.biases[node] sigmoid = torch.sigmoid(logit) children = self.tree[node] # Traverse left if sigmoid \u0026gt; 0.5, right otherwise node = children[0] if sigmoid \u0026gt; 0.5 else children[1] if len(children) \u0026gt; 1 else children[0] predictions.append(node - self.num_internal) # Convert to class index return torch.tensor(predictions, device=x.device) ","date":"March 9 2025","externalUrl":null,"permalink":"/posts/activation-functions/","section":"Posts","summary":"","title":"Activation Functions","type":"posts"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/activation-functions/","section":"Tags","summary":"","title":"Activation Functions","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/neural-networks/","section":"Tags","summary":"","title":"Neural Networks","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/relu/","section":"Tags","summary":"","title":"ReLU","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/","section":"Siddanth Emani Blog","summary":"","title":"Siddanth Emani Blog","type":"page"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/sigmoid/","section":"Tags","summary":"","title":"Sigmoid","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/softmax/","section":"Tags","summary":"","title":"Softmax","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"March 9 2025","externalUrl":null,"permalink":"/tags/tanh/","section":"Tags","summary":"","title":"Tanh","type":"tags"},{"content":"","date":"March 5 2025","externalUrl":null,"permalink":"/tags/binary-classification/","section":"Tags","summary":"","title":"Binary Classification","type":"tags"},{"content":" Introduction # Binary Cross-Entropy (BCE) loss is a cornerstone of binary classification tasks in machine learning. However, its standard implementation can encounter numerical instability when dealing with very large or small logits. This post walks through the implementation of a numerically stable BCE loss function in PyTorch, ensuring robustness during model training.\nMathematical Background # The standard BCE loss for a single sample is defined as:\n$$\\text{BCE}(z, y) = - \\left[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) \\right]$$\nwhere \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\) is the sigmoid of the logit \\( z \\), and \\( y \\) is the true label (0 or 1).\nUnderflow # Direct computation of \\( p \\) can lead to overflow or underflow for large \\( |z| \\). A numerically stable alternative is:\n$$\\text{BCE}(z, y) = \\max(z, 0) - y \\cdot z + \\log(1 + e^{-|z|})$$\nThis formulation avoids computing \\( \\sigma(z) \\) directly, mitigating numerical issues.\nCode # def bce_loss(logits: torch.Tensor, targets: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Numerically stable binary cross-entropy loss. :param logits: Raw model outputs (logits), shape (batch_size,) or (batch_size, 1) :param targets: Ground truth labels (0 or 1), same shape as logits :return: Mean loss over the batch \u0026#34;\u0026#34;\u0026#34; # Ensure logits are 1D logits = logits.squeeze() # Compute stable BCE: max(logits, 0) - logits * targets + log(1 + exp(-abs(logits))) loss = torch.maximum(logits, torch.zeros_like(logits)) - logits * targets + torch.log1p(torch.exp(-torch.abs(logits))) return loss.mean() The implementation of the BCE loss function is crucial for understanding how to handle numerical stability during training :\nInput Handling: The squeeze() operation ensures logits are a 1D tensor, accommodating varying input shapes.\nStable Computation: The formula leverages torch.maximum and torch.log1p (log(1 + x) for small x) to prevent overflow/underflow.\nBatch Averaging: The mean loss is returned, suitable for optimization. This implementation is critical for training models where logits may vary widely in magnitude, ensuring numerical reliability.\nApplications: Binary classification tasks, including emotion detection, fraud detection, and medical diagnosis.\nPerformance Considerations: While this implementation is stable, batch size and input distribution can significantly affect training dynamics.\n","date":"March 5 2025","externalUrl":null,"permalink":"/posts/loss-functions/binary-cross-entropy/","section":"Posts","summary":"","title":"Binary Cross-Entropy Loss","type":"posts"},{"content":"","date":"March 5 2025","externalUrl":null,"permalink":"/tags/loss-functions/","section":"Tags","summary":"","title":"Loss Functions","type":"tags"},{"content":"","date":"March 5 2025","externalUrl":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"Supervised Learning","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification","type":"tags"},{"content":"","date":"February 14 2025","externalUrl":null,"permalink":"/tags/linear-models/","section":"Tags","summary":"","title":"Linear Models","type":"tags"},{"content":" A classification technique which computes the probability or likelihood that the data belongs to a class of interest. $$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n$$\nGoal : Maximize likelihood\nOptimization Method : Gradient Descent\nGenerally fits an ‚ÄòS‚Äô curve (‚ÄòS‚Äô shaped logistic function) for the data to belong to separate classes.\nBinary Classification # Sigmoid transforms the linear combination of inputs into probability. $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n\\(z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\\) The curve which gives the maximum likelihood is selected as the end result.\nMultinomial Logistic Regression # One-vs-Rest (OvR) Classification :\nFor \\(K\\) classes, train \\(K\\) separate binary classifiers (one class vs the rest). For a new observation, output \\(K\\) probabilities and select the class with the highest probability. Different binary classifiers may produce overlapping decision regions which may lead to ambiguous classification, especially when classes are imbalanced. Softmax Logistic Function :\nSoftmax ensures the predicted probabilities of all classes sum to 1. $$P(y=k|x)=\\frac{e^{x^T\\beta_{k}}}{\\sum_{j=1}^Ke^{x^T\\beta_{j}}}$$\n\\(x\\) = feature vector \\(K\\) = number of classes \\(\\beta_{k}\\) = parameter vector for class \\(k\\) Provides a coherent probability model. Model coefficients can be interpreted as the change in log-odds of being in a given class. Maximum Likelihood Estimation # Binary Classification : $$L(\\beta) = \\prod_{i=1}^{N} p_i^{y_i} (1-p_i)^{1-y_i}$$\n\\(p_i\\) = predicted probability for observation (\\phi) \\(y_i\\) = actual label (0 or 1) Log Transformation (Log-Likelihood) :\nThis results in underflow for large datasets since all values are between 0 and 1. We apply log transformation to change it into a summation. $$\\ell(\\beta)=\\sum_{i=1}^{N}(y_i \\log(p_i) + (1-y_i) \\log(1-p_i))$$ Multi-Classification using Softmax : $$\\ell({\\beta_{k}}) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log\\left( \\frac{e^{x_i^\\top \\beta_{k}}}{\\sum_{j=1}^{K} e^{x_i^\\top \\beta_j}} \\right)$$\nAssumptions # An S shaped curve is assumed to fit the data. The independent variable should have strong correlation with the likelihood of the class. Linearity in the Logit : The log-odds (logits) of the outcome are assumed to be a linear combination of the independent variables. $$\\log\\left( \\frac{p}{1-p} \\right)=\\beta_{0}+\\beta_{1}x_{1}+\\dots+\\beta_{n}x_{n}$$ The coefficients are in the log-odds scale. All Linear Regression tests can be done on this line. Independence : Observations are assumed to be independent of each other. No or Little Multicollinearity : The independent variables should not be highly correlated with each other. A sufficiently large sample is preferred to obtain reliable estimates. Limitations # Linear Decision Boundary in Logit Space : Assuming a linear relationship between the predictors and the log-odds of the outcome, might not capture more complex, non-linear relationships. Sensitivity to Outliers : Extreme values can disproportionately affect the model. Multicollinearity : High correlation among independent variables can destabilize coefficient estimates. Feature transformations : For data that is not linearly separable (in the logit domain), performance may suffer unless features are transformed or interaction terms are added. Diagnostic Decision Table # Observation Implication Action Poor Overall Model Fit : High deviance, low pseudo R-squared, or significant lack-of-fit (e.g., failing the Hosmer-Lemeshow test). The model may be mis-specified, missing important predictors, or not capturing the true relationship between variables. Reassess model specification: consider adding relevant predictors, interaction terms, or non-linear transformations. Non-significant Coefficients : Predictors with high p-values or wide confidence intervals. These predictors may not contribute meaningfully to predicting the outcome, possibly diluting the model‚Äôs effectiveness. Remove or re-specify insignificant variables, and ensure that only meaningful predictors remain in the model. Low Predictive Performance : Low classification accuracy, poor ROC-AUC, or imbalanced confusion matrix (e.g., high false positives/negatives). The model might be underfitting, failing to capture important data patterns, or struggling with class imbalance. Perform feature engineering, consider re-balancing the dataset, explore alternative model specifications, or try regularization techniques. High Multicollinearity : Indicators such as high Variance Inflation Factor (VIF) values or inflated standard errors. Predictor variables are highly correlated, leading to unstable coefficient estimates and difficulty in interpreting their individual effects. Remove, combine, or transform correlated predictors, or apply regularization (L1/L2) to mitigate multicollinearity issues. Influential Observations : A few data points exhibit high Cook‚Äôs distance or leverage values. These observations may disproportionately affect model estimates, potentially skewing results and interpretations. Investigate and validate these outliers. Consider robust regression techniques or, if justified, remove problematic observations after careful analysis. Patterned Residuals : Residual plots show systematic patterns or non-random distribution. This suggests that the model is missing key relationships (e.g., interactions or non-linear effects), indicating mis-specification. Consider adding interaction terms, polynomial terms, or applying variable transformations to better capture the underlying data structure. Poor Calibration in Multi-Class Models (OvR approach) : Inconsistent or poorly calibrated predicted probabilities across classes. The individual binary classifiers may not provide directly comparable probabilities, potentially leading to ambiguous class assignments. For multi-class problems, consider switching to multinomial (softmax) logistic regression, applying probability calibration techniques, or using methods to better balance class predictions. Steps # Goal : Maximize likelihood\nModel : \\(\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\\)\nCompute the likelihood for each observation. Apply the log transformation to obtain the log-likelihood. Optimize the parameters using Gradient Descent. Calculate evaluation metrics and diagnose. Use Wald‚Äôs Test to determine whether a feature is useful in computing the prediction. Validate model assumptions such as linearity and independence. ","date":"February 14 2025","externalUrl":null,"permalink":"/posts/logistic-regression/","section":"Posts","summary":"","title":"Logistic Regression","type":"posts"},{"content":" Definition # A model that fits a linear equation to the data. $$ \\hat{y}=w^Tx+\\phi $$ where \\( w \\) = weight vector, \\( x \\) = feature vector, \\( \\phi \\) = bias\nResidual : \\(r_i=y_i-\\hat{y_{i}}=y_{i}-(w^Tx_{i}+\\phi)\\)\nLoss Function : Mean Squared Error\nOptimization Method : Gradient Descent\nAssumptions # Linearity: Dependent and independent variables have linear relationship. Check for scatterplot trends. Curved/parabolic lines indicate non-linear relationships. Independence: Observations are independent of each other. Homoscedasticity: Constant variance of residuals. Normality: Residuals are normally distributed. No Multicollinearity: Independent variables are not highly correlated. Limitations # Non-Linear Relationships : Cannot model non-linear relationships. Use polynomial regression or apply transformations. Outliers : Sensitive to outliers. Detect and remove outliers using IQR or Z-score analysis. Categorical Features : Does not handle categorical features directly. Use One-Hot Encoding for nominal features or Ordinal Encoding for ordered features. High Multicollinearity - Inflates variance and makes coefficients unreliable. Check VIF or remove redundant features using PCA or reduce overfitting using Ridge or Lasso Regression. Heteroscedasticity - Unequal variance will lead to uncertain predictions and biased hypothesis tests. Apply log transformations or WLS regression. Non Normal Residuals - Leads to unreliable p-values and confidence intervals. Apply Box-Cox transformation or use Quantile Regression High Dimensionality : Apply PCA or feature selection. Features with weak correlation may be removed. Diagnostic Decision Table # Observation Implication Action High feature-feature correlation Multicollinearity risk Calculate VIF, drop/reduce features or use Ridge Regression Strong feature-target correlation Good predictor candidate Prioritize in model. Check for data leakage Non-linear patterns Violates linearity assumption Add polynomial terms or transformations Bimodal distributions Potential subgroups in data Investigate data collection. Try separate models for each subgroup or use interactions Outliers in pairplot Potential data errors/edge cases Apply robust scaling or winsorization Heteroscedasticity Violates constant variance assumption Use log transformation or weighted regression Non-normal residuals Violates normality assumption Apply Box-Cox transformation or increase data Autocorrelation in residuals Violates independence assumption Use Durbin-Watson test; consider lag variables High leverage points Can disproportionately affect model Detect using Cook‚Äôs Distance and consider removal Omitted variable bias Model missing important predictors Include relevant features or domain knowledge Irrelevant features Increases model complexity, reduces performance Perform feature selection (LASSO, backward elimination) Steps # Goal: Minimize MSE\nCheck all assumptions. Split data into training and testing sets. Initialize random weights for each feature. Calculate predictions. Calculate MSE. Calculate steps using Gradient Descent Evaluate using \\(R^2\\): If \\(R^2\\) is low, check for missing predictor or non-linearity. If \\(R^2\\) is too high, possible overfitting (Adjusted \\(R^2\\)) Calculate p-value for \\(R^2\\) using F-Test. Plot feature coefficients, residual distribution and actual vs predicted values scatter plot. Code # import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from statsmodels.stats.outliers_influence import variance_inflation_factor from scipy.stats import shapiro, f_oneway import statsmodels.api as sm # Load dataset df = pd.read_csv(\u0026#39;winequality-red.csv\u0026#39;, sep=\u0026#39;;\u0026#39;) # Define target column target_column = \u0026#39;quality\u0026#39; # Step 1: Validate Assumptions Before Applying Linear Regression # 1.1 Check for missing values if df.isnull().sum().sum() \u0026gt; 0: print(\u0026#34;‚ö†Ô∏è Dataset contains missing values. Handle them before proceeding.\u0026#34;) # 1.2 Check multicollinearity using Variance Inflation Factor (VIF) X = df.drop(columns=[target_column]) y = df[target_column] vif_data = pd.DataFrame() vif_data[\u0026#34;Feature\u0026#34;] = X.columns vif_data[\u0026#34;VIF\u0026#34;] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] if vif_data[\u0026#34;VIF\u0026#34;].max() \u0026gt; 10: print(\u0026#34;‚ö†Ô∏è High multicollinearity detected. Consider removing/reducing features using PCA, Ridge Regression, or feature selection.\u0026#34;) # 1.3 Check linearity assumption using a correlation heatmap plt.figure(figsize=(10, 8)) sns.heatmap(df.corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Feature Correlation Heatmap\u0026#34;) plt.show() # 1.4 Check residual normality using the Shapiro-Wilk test (valid for small samples) _, p_value = shapiro(df[target_column]) if p_value \u0026lt; 0.01: print(\u0026#34;‚ö†Ô∏è Residuals are not normally distributed. Consider log transformation or robust regression.\u0026#34;) # 1.5 Check homoscedasticity using ANOVA _, p_homo = f_oneway(*[df[target_column][df[col] \u0026gt; df[col].median()] for col in df.drop(columns=[target_column]).columns]) if p_homo \u0026lt; 0.05: print(\u0026#34;‚ö†Ô∏è Heteroscedasticity detected. Consider applying log transformation or using Weighted Least Squares (WLS).\u0026#34;) print(\u0026#34;Proceeding with Linear Regression...\u0026#34;) # Step 2: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Step 3: Train Linear Regression Model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Predictions y_pred = model.predict(X_test) # Step 5: Evaluate Model rmse = np.sqrt(mean_squared_error(y_test, y_pred)) r2 = r2_score(y_test, y_pred) print(f\u0026#34;\\nüìä Model Performance:\\nRMSE: {rmse:.3f}\\nR-squared (R¬≤): {r2:.3f}\u0026#34;) # Step 6: Feature Importance feature_importance = pd.DataFrame({\u0026#39;Feature\u0026#39;: X_train.columns, \u0026#39;Coefficient\u0026#39;: model.coef_}) feature_importance = feature_importance.sort_values(by=\u0026#39;Coefficient\u0026#39;, ascending=False) plt.figure(figsize=(10, 5)) sns.barplot(x=\u0026#39;Coefficient\u0026#39;, y=\u0026#39;Feature\u0026#39;, data=feature_importance, palette=\u0026#39;coolwarm\u0026#39;) plt.title(\u0026#34;Feature Importance in Linear Regression\u0026#34;) plt.show() # Step 7: Residual Analysis residuals = y_test - y_pred plt.figure(figsize=(8, 5)) sns.histplot(residuals, kde=True, bins=30) plt.title(\u0026#34;Residuals Distribution\u0026#34;) plt.xlabel(\u0026#34;Residuals\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_pred, y=residuals, alpha=0.5) plt.axhline(y=0, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#34;Predicted Quality\u0026#34;) plt.ylabel(\u0026#34;Residuals\u0026#34;) plt.title(\u0026#34;Residuals vs Predicted Values\u0026#34;) plt.show() # Step 8: Actual vs Predicted Visualization plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_test, y=y_pred, alpha=0.6) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Perfect fit line plt.xlabel(\u0026#34;Actual Quality\u0026#34;) plt.ylabel(\u0026#34;Predicted Quality\u0026#34;) plt.title(\u0026#34;Actual vs. Predicted Wine Quality\u0026#34;) plt.show() # Step 9: Model Statistics (p-value for R¬≤) X_train_sm = sm.add_constant(X_train) # Add constant for intercept ols_model = sm.OLS(y_train, X_train_sm).fit() print(\u0026#34;\\nüìä Model Summary:\u0026#34;) print(ols_model.summary()) # If R¬≤ is too high, check for overfitting adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1) if adjusted_r2 \u0026lt; r2 - 0.05: print(\u0026#34;‚ö†Ô∏è Possible overfitting detected. Consider cross-validation or feature selection.\u0026#34;) ","date":"January 30 2025","externalUrl":null,"permalink":"/posts/linear-regression/","section":"Posts","summary":"","title":"Linear Regression","type":"posts"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/regression/","section":"Tags","summary":"","title":"Regression","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/resnet/","section":"Tags","summary":"","title":"ResNet","type":"tags"},{"content":"","externalUrl":null,"permalink":"/about/","section":"Siddanth Emani Blog","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]