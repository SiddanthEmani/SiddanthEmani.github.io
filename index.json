[{"content":" Definition # A model that fits a linear equation to the data. $$ \\hat{y}=w^Tx+\\phi $$ where \\( w \\) = weight vector, \\( x \\) = feature vector, \\( \\phi \\) = bias\nResidual : \\(r_i=y_i-\\hat{y_{i}}=y_{i}-(w^Tx_{i}+\\phi)\\)\nLoss Function : Mean Squared Error\nOptimization Method : Gradient Descent\nAssumptions # Linearity: Dependent and independent variables have linear relationship. Check for scatterplot trends. Curved/parabolic lines indicate non-linear relationships. Independence: Observations are independent of each other. Homoscedasticity: Constant variance of residuals. Normality: Residuals are normally distributed. No Multicollinearity: Independent variables are not highly correlated. Limitations # Non-Linear Relationships : Cannot model non-linear relationships. Use polynomial regression or apply transformations. Outliers : Sensitive to outliers. Detect and remove outliers using IQR or Z-score analysis. Categorical Features : Does not handle categorical features directly. Use One-Hot Encoding for nominal features or Ordinal Encoding for ordered features. High Multicollinearity - Inflates variance and makes coefficients unreliable. Check VIF or remove redundant features using PCA or reduce overfitting using Ridge or Lasso Regression. Heteroscedasticity - Unequal variance will lead to uncertain predictions and biased hypothesis tests. Apply log transformations or WLS regression. Non Normal Residuals - Leads to unreliable p-values and confidence intervals. Apply Box-Cox transformation or use Quantile Regression High Dimensionality : Apply PCA or feature selection. Features with weak correlation may be removed. Diagnostic Decision Table # Observation Implication Action High feature-feature correlation Multicollinearity risk Calculate VIF, drop/reduce features or use Ridge Regression Strong feature-target correlation Good predictor candidate Prioritize in model. Check for data leakage Non-linear patterns Violates linearity assumption Add polynomial terms or transformations Bimodal distributions Potential subgroups in data Investigate data collection. Try separate models for each subgroup or use interactions Outliers in pairplot Potential data errors/edge cases Apply robust scaling or winsorization Heteroscedasticity Violates constant variance assumption Use log transformation or weighted regression Non-normal residuals Violates normality assumption Apply Box-Cox transformation or increase data Autocorrelation in residuals Violates independence assumption Use Durbin-Watson test; consider lag variables High leverage points Can disproportionately affect model Detect using Cook‚Äôs Distance and consider removal Omitted variable bias Model missing important predictors Include relevant features or domain knowledge Irrelevant features Increases model complexity, reduces performance Perform feature selection (LASSO, backward elimination) Steps # Goal: Minimize MSE\nCheck all assumptions. Split data into training and testing sets. Initialize random weights for each feature. Calculate predictions. Calculate MSE. Calculate steps using Gradient Descent Evaluate using \\(R^2\\): If \\(R^2\\) is low, check for missing predictor or non-linearity. If \\(R^2\\) is too high, possible overfitting (Adjusted \\(R^2\\)) Calculate p-value for \\(R^2\\) using F-Test. Plot feature coefficients, residual distribution and actual vs predicted values scatter plot. Code # import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from statsmodels.stats.outliers_influence import variance_inflation_factor from scipy.stats import shapiro, f_oneway import statsmodels.api as sm # Load dataset df = pd.read_csv(\u0026#39;winequality-red.csv\u0026#39;, sep=\u0026#39;;\u0026#39;) # Define target column target_column = \u0026#39;quality\u0026#39; # Step 1: Validate Assumptions Before Applying Linear Regression # 1.1 Check for missing values if df.isnull().sum().sum() \u0026gt; 0: print(\u0026#34;‚ö†Ô∏è Dataset contains missing values. Handle them before proceeding.\u0026#34;) # 1.2 Check multicollinearity using Variance Inflation Factor (VIF) X = df.drop(columns=[target_column]) y = df[target_column] vif_data = pd.DataFrame() vif_data[\u0026#34;Feature\u0026#34;] = X.columns vif_data[\u0026#34;VIF\u0026#34;] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] if vif_data[\u0026#34;VIF\u0026#34;].max() \u0026gt; 10: print(\u0026#34;‚ö†Ô∏è High multicollinearity detected. Consider removing/reducing features using PCA, Ridge Regression, or feature selection.\u0026#34;) # 1.3 Check linearity assumption using a correlation heatmap plt.figure(figsize=(10, 8)) sns.heatmap(df.corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Feature Correlation Heatmap\u0026#34;) plt.show() # 1.4 Check residual normality using the Shapiro-Wilk test (valid for small samples) _, p_value = shapiro(df[target_column]) if p_value \u0026lt; 0.01: print(\u0026#34;‚ö†Ô∏è Residuals are not normally distributed. Consider log transformation or robust regression.\u0026#34;) # 1.5 Check homoscedasticity using ANOVA _, p_homo = f_oneway(*[df[target_column][df[col] \u0026gt; df[col].median()] for col in df.drop(columns=[target_column]).columns]) if p_homo \u0026lt; 0.05: print(\u0026#34;‚ö†Ô∏è Heteroscedasticity detected. Consider applying log transformation or using Weighted Least Squares (WLS).\u0026#34;) print(\u0026#34;Proceeding with Linear Regression...\u0026#34;) # Step 2: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Step 3: Train Linear Regression Model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Predictions y_pred = model.predict(X_test) # Step 5: Evaluate Model rmse = np.sqrt(mean_squared_error(y_test, y_pred)) r2 = r2_score(y_test, y_pred) print(f\u0026#34;\\nüìä Model Performance:\\nRMSE: {rmse:.3f}\\nR-squared (R¬≤): {r2:.3f}\u0026#34;) # Step 6: Feature Importance feature_importance = pd.DataFrame({\u0026#39;Feature\u0026#39;: X_train.columns, \u0026#39;Coefficient\u0026#39;: model.coef_}) feature_importance = feature_importance.sort_values(by=\u0026#39;Coefficient\u0026#39;, ascending=False) plt.figure(figsize=(10, 5)) sns.barplot(x=\u0026#39;Coefficient\u0026#39;, y=\u0026#39;Feature\u0026#39;, data=feature_importance, palette=\u0026#39;coolwarm\u0026#39;) plt.title(\u0026#34;Feature Importance in Linear Regression\u0026#34;) plt.show() # Step 7: Residual Analysis residuals = y_test - y_pred plt.figure(figsize=(8, 5)) sns.histplot(residuals, kde=True, bins=30) plt.title(\u0026#34;Residuals Distribution\u0026#34;) plt.xlabel(\u0026#34;Residuals\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_pred, y=residuals, alpha=0.5) plt.axhline(y=0, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#34;Predicted Quality\u0026#34;) plt.ylabel(\u0026#34;Residuals\u0026#34;) plt.title(\u0026#34;Residuals vs Predicted Values\u0026#34;) plt.show() # Step 8: Actual vs Predicted Visualization plt.figure(figsize=(8, 5)) sns.scatterplot(x=y_test, y=y_pred, alpha=0.6) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Perfect fit line plt.xlabel(\u0026#34;Actual Quality\u0026#34;) plt.ylabel(\u0026#34;Predicted Quality\u0026#34;) plt.title(\u0026#34;Actual vs. Predicted Wine Quality\u0026#34;) plt.show() # Step 9: Model Statistics (p-value for R¬≤) X_train_sm = sm.add_constant(X_train) # Add constant for intercept ols_model = sm.OLS(y_train, X_train_sm).fit() print(\u0026#34;\\nüìä Model Summary:\u0026#34;) print(ols_model.summary()) # If R¬≤ is too high, check for overfitting adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1) if adjusted_r2 \u0026lt; r2 - 0.05: print(\u0026#34;‚ö†Ô∏è Possible overfitting detected. Consider cross-validation or feature selection.\u0026#34;) ","date":"January 30 2025","externalUrl":null,"permalink":"/posts/linear-regression/","section":"Posts","summary":"","title":"Linear Regression","type":"posts"},{"content":" ResNet Block # 3x3 Convolution (stride=2) - Downsamples the image into half the height and width. Also, doubles the number of channels (number of filters are doubled). Sliding a 3x3 kernel/matrix onto the image and dot product is calculated with overlapped images. 3x3 is small enough to capture fine details and more efficient. Can be stacked to capture complex patterns. Batch Norm ReLU Thought Process # Our goal is to increase the resolution of the image.\nIf the model is trained on a vanilla deep net with both low and high resolution input images, it will increase our training loss since the input signal is being lost when its passed through a lot of layers.\nOur goal is to now learn the residual to be added to the image to increase its resolution.\nThe idea is to add the input image to our residual at the end of our block. Two problems arise:\nDimension mismatch (When the input and residual have different dimensions, we cannot add) A steady stream of input data across the network So we add residual connections to handle these. And we treat the network as a series of residual blocks instead of layers.\nEach block does not take penalty from the loss function since it can output the same identity function.\nThis enables very deep networks.\nCode # def forward(self, x: Tensor) -\u0026gt; Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out Resnet18, 34, 50, 101, 152. These are pretrained models and the number indicates the number of layers in the architecture.\nArchitecture # ResNet models introduce the concept of residual learning, where the network learns residual functions with reference to the layer inputs, rather than learning unreferenced functions. This allows the model to train very deep networks effectively.\nUse Cases # ResNet models are commonly used in image classification tasks and are known for their performance on large-scale datasets such as ImageNet.\nStrengths and Weaknesses # Strengths: Effective in training very deep networks. Reduces the problem of vanishing gradients. Weaknesses: Computationally expensive for very deep versions like ResNet-152. May not be as efficient for smaller datasets or less complex tasks. Papers # He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778*. ","date":"January 9 2025","externalUrl":null,"permalink":"/posts/resnet/","section":"Posts","summary":"","title":"What is ResNet?","type":"posts"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/linear-models/","section":"Tags","summary":"","title":"Linear Models","type":"tags"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/regression/","section":"Tags","summary":"","title":"Regression","type":"tags"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/","section":"Siddanth Emani Blog","summary":"","title":"Siddanth Emani Blog","type":"page"},{"content":"","date":"January 30 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"January 9 2025","externalUrl":null,"permalink":"/tags/resnet/","section":"Tags","summary":"","title":"ResNet","type":"tags"},{"content":"","externalUrl":null,"permalink":"/about/","section":"Siddanth Emani Blog","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]